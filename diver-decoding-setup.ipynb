{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c728e87-91ae-4af4-96eb-246573b0516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import metrics\n",
    "from plot_utils import "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f634ea-2c4a-4d0b-82d9-79044001a904",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3ffd89-7d2e-4ad5-9e60-dfc19480ecf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to map names to metrics. Can configure to use these how you would like. See below training configuration.\n",
    "simple_metric_dict = {\n",
    "    \"mse\": metrics.mse,\n",
    "    \"nll_embedding\": metrics.compute_nll_contextual,\n",
    "    \"cosine_sim\": metrics.cosine_similarity,\n",
    "    \"cosine_distance\": metrics.cosine_distance,\n",
    "    \"similarity_entropy\": metrics.similarity_entropy,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acc881cc-c84b-48f7-99cc-2fe4999efff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration\n",
    "# Time relative to word onset to center neural data around. In ms.\n",
    "lag: int = 0\n",
    "# The width of neural data to gather around each word onset in seconds.\n",
    "window_width: float = 0.5\n",
    "# The name of the embeddings to use. Currently supports gpt-2xl and arbitrary.\n",
    "embedding_type: str = \"gpt-2xl\"\n",
    "# Layer of model to gather embeddings from. Required if using gpt2-xl.\n",
    "embedding_layer: Optional[int] = None\n",
    "# Root of data folder.\n",
    "data_root: str = \"data\"\n",
    "# Number of embeddings to reduce the embeddings to using pca. If None, don't run PCA.\n",
    "embedding_pca_dim: Optional[int] = None\n",
    "# CSV file with columns subject (subject integer id) and elec (string name of electrode).\n",
    "# If not set then defaults to configured subject_ids and channel_reg_ex. See our significant electrode file.\n",
    "electrode_file_path: Optional[str] = None\n",
    "# The subject id's to include in your analysis. For the podcast data they must all be in the range [1, 9]\n",
    "subject_ids: list[int] = []\n",
    "# A regular expression to pick which channels you are interested in.\n",
    "# (i.e. \"LG[AB]*\" will select channels that start with \"LGA\" or \"LGB\")\n",
    "channel_reg_ex: Optional[str] = None\n",
    "# Column name in dataframe to use for grouping by words.\n",
    "word_column: Optional[str] = \"lemmatized_word\"\n",
    "# Used in preprocessor specific to the PITOM model. See preprocess_neural_data() below for overwriting with your own.\n",
    "num_average_samples: int = 32 \n",
    "\n",
    "# Training Configuration\n",
    "# The batch size to train our decoder with.\n",
    "batch_size: int = 32\n",
    "# The maximum number of epochs to train over each fold with.\n",
    "epochs: int = 100\n",
    "# The learning rate to use when training. TODO: currently staic lr, could use a scheduler in the future.\n",
    "learning_rate: float = 0.001\n",
    "# The amount of weight decay to use as regularization in our optimizer.\n",
    "weight_decay: float = 0.0001\n",
    "# If cosine similarity between our predicted embeddings and the actual embeddings do not improve after this many steps\n",
    "# stop training for this fold early.\n",
    "early_stopping_patience: int = 10\n",
    "# Number of folds to train over per-lag.\n",
    "n_folds: int = 5\n",
    "# Path to write model checkpoints to.\n",
    "model_dir: str = \"models\"\n",
    "# Type of fold generation to use. One of \"sequential_folds\" or \"zero_shot_folds\". Sequential folds mean we\n",
    "# use time segmented folds. zero_shot_folds requires that no word in the test set appears in the training set.\n",
    "fold_type: str = \"sequential_folds\"\n",
    "# Losses to use, by default use nll_embedding. Must be defined in simple_metric_dict. We find that nll_embedding\n",
    "# tends to be the best for our decoding tests because it is a contrastive loss (roughly equivalent to negative log likelihood over a batch)\n",
    "losses: list[str] = [\"nll_embedding\"]\n",
    "# Weight to assign to each loss. Should be parallel array with losses.\n",
    "loss_weights: list[float] = [1.0]\n",
    "# Metrics to track during training. Must be defined in simple_metric_dict.\n",
    "metrics: list[str] = [\"mse\", \"cosine_sim\"]\n",
    "# Metric to use for early stopping over validation set. Must be either the loss or in metrics.\n",
    "early_stopping_metric: str = \"cosine_sim\"\n",
    "# Whether or not a smaller value is better for early_stopping_metric. Should be False for metrics you\n",
    "# want to increase (i.e. cosine similarity) but True for ones you want to decrease (i.e. MSE).\n",
    "smaller_is_better: bool = False\n",
    "# Number of gradient accumulation steps.\n",
    "grad_accumulation_steps: int = 1\n",
    "# TODO: Generalize parameters to metrics based on config. So we don't need to have these last few.\n",
    "# Minimum number of occurences of a word in training set to be used for ROC-AUC calculation.\n",
    "min_train_freq_auc: int = 5\n",
    "# Minimum number of occurences of a word in test set to be used for ROC-AUC calculation.\n",
    "min_test_freq_auc: int = -1\n",
    "# Sets the k we use in top-k metrics.\n",
    "top_k_thresholds: list[int] = [1, 5, 10]\n",
    "\n",
    "# Model configuration (specific to your model). Passed in the get_model function below.\n",
    "model_params = {\n",
    "  \"conv_filters\": 128\n",
    "  \"reg\": 0.35\n",
    "  \"reg_head\": 0\n",
    "  \"dropout\": 0.2\n",
    "  \"num_models\": 10\n",
    "  \"embedding_dim\": 50\n",
    "}\n",
    "\n",
    "# Other configuration\n",
    "# Base directory to output results to.\n",
    "output_dir: str = \"results\"\n",
    "# Base directory to write models to.\n",
    "model_dir: str = \"models\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c59a159-dd8a-4fd0-a900-785208b471dd",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c951b5-3361-4e08-ba5c-e6a99aecf3de",
   "metadata": {},
   "source": [
    "You can replace this with your own DIVER model code here, or just import from your own files.\n",
    "\n",
    "This model is roughly the one used for decoding in the 2022 paper: https://www.nature.com/articles/s41593-022-01026-4#Sec31. If you have your model setup to return a predicted word embedding, the EmbeddingPrediction class below may be useful for translating that into a prediction over all of the words as specified in the Decoding analysis section in the paper above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d70dbd6-878a-4499-ad1c-ec7461aa055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PitomModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels,\n",
    "        output_dim,\n",
    "        conv_filters=128,\n",
    "        reg=0.35,\n",
    "        reg_head=0,\n",
    "        dropout=0.2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        PyTorch implementation of the PITOM decoding model.\n",
    "        \n",
    "        Args:\n",
    "            input_channels: Numbr of electrodes in data (int)\n",
    "            output_dim: Dimension of output vector (int)\n",
    "            conv_filters: Number of convolutional filters (default: 128)\n",
    "            reg: L2 regularization factor for convolutional layers (default: 0.35)\n",
    "            reg_head: L2 regularization factor for dense head (default: 0)\n",
    "            dropout: Dropout rate (default: 0.2)\n",
    "        \"\"\"\n",
    "        super(PitomModel, self).__init__()\n",
    "        \n",
    "        self.conv_filters = conv_filters\n",
    "        self.reg = reg\n",
    "        self.reg_head = reg_head\n",
    "        self.dropout = dropout\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Define the CNN architecture\n",
    "        self.desc = [(conv_filters, 3), ('max', 2), (conv_filters, 2)]\n",
    "        \n",
    "        # Build the layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i, (filters, kernel_size) in enumerate(self.desc):\n",
    "            if filters == 'max':\n",
    "                self.layers.append(\n",
    "                    nn.MaxPool1d(kernel_size=kernel_size, stride=kernel_size, padding=kernel_size//2)\n",
    "                )\n",
    "            else:\n",
    "                # Conv block\n",
    "                conv = nn.Conv1d(\n",
    "                    in_channels=input_channels if i == 0 else conv_filters,\n",
    "                    out_channels=filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=1,\n",
    "                    padding=0,  # 'valid' in Keras\n",
    "                    bias=False\n",
    "                )\n",
    "                \n",
    "                # Apply weight decay equivalent to L2 regularization\n",
    "                self.layers.append(conv)\n",
    "                self.layers.append(nn.ReLU())\n",
    "                self.layers.append(nn.BatchNorm1d(filters))\n",
    "                self.layers.append(nn.Dropout(dropout))\n",
    "                \n",
    "                input_channels = filters\n",
    "        \n",
    "        # Final locally connected layer (using Conv1d with groups as approximation).\n",
    "        # Not exactly the same as original paper but pytorch does not have locally connected layers.\n",
    "        self.final_conv = nn.Conv1d(\n",
    "            in_channels=conv_filters,\n",
    "            out_channels=conv_filters,\n",
    "            kernel_size=2,\n",
    "            stride=1,\n",
    "            padding=0,  # 'valid' in Keras\n",
    "            bias=True\n",
    "        )\n",
    "        \n",
    "        self.final_bn = nn.BatchNorm1d(conv_filters)\n",
    "        self.final_act = nn.ReLU()\n",
    "        \n",
    "        # Output layer\n",
    "        self.dense = nn.Linear(conv_filters, output_dim)\n",
    "        self.layer_norm = nn.LayerNorm(output_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        x = self.final_conv(x)\n",
    "        x = self.final_bn(x)\n",
    "        x = self.final_act(x)\n",
    "        \n",
    "        x = F.adaptive_max_pool1d(x, 1).squeeze(-1)\n",
    "        \n",
    "        x = self.dense(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.tanh(x)\n",
    "            \n",
    "        return x\n",
    "\n",
    "\n",
    "class EnsemblePitomModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_models: int,\n",
    "        input_channels,\n",
    "        output_dim: int,\n",
    "        conv_filters=128,\n",
    "        reg=0.35,\n",
    "        reg_head=0,\n",
    "        dropout=0.2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        PyTorch implementation of the PITOM decoding model.\n",
    "        \n",
    "        Args:\n",
    "            num_models: The number of models to include in the ensemble. The outputs will be averaged at the end.\n",
    "            input_channels: Numbr of electrodes in data (int)\n",
    "            output_dim: Dimensionality of output (int)\n",
    "            conv_filters: Number of convolutional filters (default: 128)\n",
    "            reg: L2 regularization factor for convolutional layers (default: 0.35)\n",
    "            reg_head: L2 regularization factor for dense head (default: 0)\n",
    "            dropout: Dropout rate (default: 0.2)\n",
    "        \"\"\"\n",
    "        super(EnsemblePitomModel, self).__init__()\n",
    "\n",
    "        self.models = nn.ModuleList()\n",
    "        for _ in range(num_models):\n",
    "            self.models.append(PitomModel(\n",
    "                input_channels,\n",
    "                output_dim,\n",
    "                conv_filters=conv_filters,\n",
    "                reg=reg,\n",
    "                reg_head=reg_head,\n",
    "                dropout=dropout\n",
    "            ))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run all models and average together all embeddings.\n",
    "        embeddings = torch.stack([model(x) for model in self.models])\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "# Make sure to update this get_model function here which takes in your configured model parameters above.\n",
    "def get_model(model_params):\n",
    "    return EnsemblePitomModel(**model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa3680b-8fc0-407f-bc09-e68b1b1ef247",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de06f5b-eacb-4e06-a916-44f47b17bb53",
   "metadata": {},
   "source": [
    "Make sure you've run ./setup.sh to download the dataset to your machine. If you want it to create a venv with gpu dependencies you can run ./setup.sh --gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cacb8c2-e323-44f4-8252-d1befdb6d37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite this to preprocess the data as needed for your model. Will be passed straight into your decoding model in this form.\n",
    "# see below for where it is called.\n",
    "def preprocess_neural_data(data, preprocessor_params):\n",
    "    return data.reshape(\n",
    "        data.shape[0], data.shape[1], -1, num_average_samples\n",
    "    ).mean(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a73e3e9-3a6a-41e8-9cd4-a7ae8596594f",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a83c68d-be3b-4717-b5e2-dac5a58be8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt_2xl_embeddings(df_contextual):\n",
    "    \"\"\"\n",
    "    Loads GPT-2 XL contextual embeddings and aligns them to word-level units.\n",
    "\n",
    "    This function:\n",
    "    1. Loads sub-token-level GPT-2 XL embeddings from a specified HDF5 file.\n",
    "    2. Groups the embeddings according to word indices provided in the contextual DataFrame.\n",
    "    3. Averages sub-token embeddings to produce a single embedding vector per word.\n",
    "\n",
    "    Args:\n",
    "        df_contextual (pd.DataFrame): DataFrame containing token-level data, including `word_idx` for grouping.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A 2D array of shape (num_words, embedding_dim), where each row is a word-level embedding.\n",
    "    \"\"\"\n",
    "    embedding_path = os.path.join(\n",
    "        data_root, \"stimuli/gpt2-xl/features.hdf5\"\n",
    "    )\n",
    "\n",
    "    with h5py.File(embedding_path, \"r\") as f:\n",
    "        contextual_embeddings = f[f\"layer-{embedding_layer}\"][...]\n",
    "\n",
    "    # Group embeddings for each word (some are sub-tokenized).\n",
    "    aligned_embeddings = []\n",
    "    for _, group in df_contextual.groupby(\"word_idx\"):  # group by word index\n",
    "        indices = group.index.to_numpy()\n",
    "        average_emb = contextual_embeddings[indices].mean(0)  # average features\n",
    "        aligned_embeddings.append(average_emb)\n",
    "    aligned_embeddings = np.stack(aligned_embeddings)\n",
    "\n",
    "    return aligned_embeddings\n",
    "\n",
    "def get_arbitrary_embeddings(df_word):\n",
    "    \"\"\"\n",
    "    Generates arbitrary (random) embeddings for each unique word in the input DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_word : pandas.DataFrame\n",
    "        A DataFrame containing a column named 'word', representing a list of words.\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame:\n",
    "        df_word with arbitrary embeddings in embedding column\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - Embeddings are randomly sampled from a uniform distribution in the range [-1.0, 1.0]\n",
    "      with an initial dimensionality of 50, then truncated or padded to match\n",
    "      `embedding_pca_dim`.\n",
    "    - Useful as a placeholder or for testing models where real word embeddings are not required.\n",
    "    \"\"\"\n",
    "    words = df_word.word.tolist()\n",
    "    unique_words = list(set(words))\n",
    "    word_to_idx = {}\n",
    "    for i, word in enumerate(words):\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = []\n",
    "        word_to_idx[word].append(i)\n",
    "\n",
    "    arbitrary_embeddings_per_word = np.random.uniform(\n",
    "        low=-1.0, high=1.0, size=(len(unique_words), embedding_pca_dim)\n",
    "    )\n",
    "    arbitrary_embeddings = np.zeros((len(words), embedding_pca_dim))\n",
    "    for i, word in enumerate(unique_words):\n",
    "        for idx in word_to_idx[word]:\n",
    "            arbitrary_embeddings[idx] = arbitrary_embeddings_per_word[i]\n",
    "\n",
    "    df_word[\"target\"] = list(arbitrary_embeddings)\n",
    "\n",
    "    return df_word\n",
    "\n",
    "def word_embedding_decoding_task():\n",
    "    \"\"\"\n",
    "    Loads and processes word-level data and retrieves corresponding embeddings based on specified parameters.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Loads a transcript file containing token-level information.\n",
    "    2. Retrieves aligned embeddings for each token or word, depending on the specified embedding type.\n",
    "    3. Groups sub-token entries into full words using word indices.\n",
    "    4. Optionally applies PCA to reduce the dimensionality of the embeddings.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, np.ndarray]: A DataFrame containing word-level information (word, start time, end time),\n",
    "        and a NumPy array of corresponding word-level embeddings under the header target.\n",
    "    \"\"\"\n",
    "    import nltk\n",
    "    from nltk.stem import WordNetLemmatizer as wl\n",
    "\n",
    "    try:\n",
    "        nltk.data.find(\"corpora/wordnet\")\n",
    "        print(\"WordNet already downloaded\")\n",
    "    except LookupError:\n",
    "        print(\"Downloading WordNet...\")\n",
    "        nltk.download(\"wordnet\")\n",
    "        nltk.download(\"wordnet\")\n",
    "\n",
    "    transcript_path = os.path.join(\n",
    "        data_root, \"stimuli/gpt2-xl/transcript.tsv\"\n",
    "    )\n",
    "\n",
    "    # Load transcript\n",
    "    df_contextual = pd.read_csv(transcript_path, sep=\"\\t\", index_col=0)\n",
    "\n",
    "    aligned_embeddings = get_gpt_2xl_embeddings(\n",
    "        df_contextual\n",
    "    )\n",
    "\n",
    "    # Group sub-tokens together into words.\n",
    "    df_word = df_contextual.groupby(\"word_idx\").agg(\n",
    "        dict(word=\"first\", start=\"first\", end=\"last\")\n",
    "    )\n",
    "    df_word[\"norm_word\"] = df_word.word.str.lower().str.replace(\n",
    "        r\"^[^\\w\\s]+|[^\\w\\s]+$\", \"\", regex=True\n",
    "    )\n",
    "    df_word[\"lemmatized_word\"] = df_word.norm_word.apply(lambda x: wl().lemmatize(x))\n",
    "\n",
    "    if embedding_type == \"gpt-2xl\":\n",
    "        df_word[\"target\"] = list(aligned_embeddings)\n",
    "    elif embedding_type == \"arbitrary\":\n",
    "        df_word = get_arbitrary_embeddings(df_word)\n",
    "\n",
    "    if embedding_pca_dim:\n",
    "        pca = PCA(n_components=embedding_pca_dim, svd_solver=\"auto\")\n",
    "        df_word.target = list(pca.fit_transform(df_word.target.tolist()))\n",
    "\n",
    "    return df_word\n",
    "\n",
    "\n",
    "def get_data(\n",
    "    lag,\n",
    "    raws: list[mne.io.Raw],\n",
    "    df_word: pd.DataFrame,\n",
    "    window_width: float,\n",
    "    word_column: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"Gather data for every word in df_word from raw.\n",
    "\n",
    "    Args:\n",
    "        lag: the lag relative to each word onset to gather data around\n",
    "        raws: list of mne.Raw object holding electrode data\n",
    "        df_word: dataframe containing columns start, end, word, and target\n",
    "        window_width: the width of the window which is gathered around each word onset + lag\n",
    "        word_column: If provided, will return the column of words specified here.\n",
    "    \"\"\"\n",
    "    datas = []\n",
    "    for raw in raws:\n",
    "        # Calculate time bounds for filtering\n",
    "        tmin = lag / 1000 - window_width / 2\n",
    "        tmax = lag / 1000 + window_width / 2 - 2e-3\n",
    "        data_duration = raw.times[-1]  # End time of the data\n",
    "\n",
    "        # Filter out events where the time window falls outside data bounds\n",
    "        valid_mask = (df_word.start + tmin >= 0) & (\n",
    "            df_word.start + tmax <= data_duration\n",
    "        )\n",
    "        df_word_valid = df_word[valid_mask].reset_index(drop=True)\n",
    "\n",
    "        if len(df_word_valid) == 0:\n",
    "            # No valid events for this raw, skip\n",
    "            continue\n",
    "\n",
    "        events = np.zeros((len(df_word_valid), 3), dtype=int)\n",
    "        events[:, 0] = (df_word_valid.start * raw.info[\"sfreq\"]).astype(int)\n",
    "\n",
    "        epochs = mne.Epochs(\n",
    "            raw,\n",
    "            events,\n",
    "            tmin=tmin,\n",
    "            tmax=tmax,\n",
    "            baseline=None,\n",
    "            proj=False,\n",
    "            event_id=None,\n",
    "            preload=True,\n",
    "            on_missing=\"ignore\",\n",
    "            event_repeated=\"merge\",\n",
    "            verbose=\"ERROR\",\n",
    "        )\n",
    "\n",
    "        data = epochs.get_data(copy=False)\n",
    "        selected_targets = df_word_valid.target[epochs.selection]\n",
    "\n",
    "        # TODO: Clean this up so we don't need to pass around this potentially None variable.\n",
    "        if word_column:\n",
    "            selected_words = df_word_valid[word_column].to_numpy()[epochs.selection]\n",
    "        else:\n",
    "            selected_words = None\n",
    "\n",
    "        # Make sure the number of samples match\n",
    "        assert data.shape[0] == selected_targets.shape[0], \"Sample counts don't match\"\n",
    "        if selected_words is not None:\n",
    "            assert data.shape[0] == selected_words.shape[0], \"Words don't match\"\n",
    "\n",
    "        datas.append(data)\n",
    "\n",
    "    if len(datas) == 0:\n",
    "        raise ValueError(\"No valid events found within data time bounds\")\n",
    "\n",
    "    datas = np.concatenate(datas, axis=1)\n",
    "    # Your preprocessor is called here.\n",
    "    datas = preprocess_neural_data(datas)\n",
    "\n",
    "    return datas, selected_targets, selected_words\n",
    "\n",
    "def load_raws(per_subject_electrodes):\n",
    "    \"\"\"\n",
    "    Loads raw iEEG data for multiple subjects based on specified parameters.\n",
    "\n",
    "    This function:\n",
    "    1. Iterates over subject IDs provided in the configuration.\n",
    "    2. Constructs BIDS-compliant file paths to locate preprocessed high-gamma iEEG data.\n",
    "    3. Loads each subject's data using MNE's `read_raw_fif` function.\n",
    "    4. Optionally filters channels using a regular expression (e.g., for selecting specific electrode groups).\n",
    "    5. Collects and returns a list of raw MNE objects.\n",
    "\n",
    "    Args:\n",
    "        per_subject_electrodes: dictionary mapping subject ID's to a list of string names for the electrodes to use.\n",
    "\n",
    "    Returns:\n",
    "        List[mne.io.Raw]: A list of raw iEEG recordings for the specified subjects.\n",
    "    \"\"\"\n",
    "    raws = []\n",
    "    for sub_id in subject_ids:\n",
    "        file_path = BIDSPath(\n",
    "            root=os.path.join(data_root, \"derivatives/ecogprep\"),\n",
    "            subject=f\"{sub_id:02}\",\n",
    "            task=\"podcast\",\n",
    "            datatype=\"ieeg\",\n",
    "            description=\"highgamma\",\n",
    "            suffix=\"ieeg\",\n",
    "            extension=\".fif\",\n",
    "        )\n",
    "\n",
    "        raw = mne.io.read_raw_fif(file_path, verbose=False)\n",
    "        if per_subject_electrodes:\n",
    "            subject_electrode_names = per_subject_electrodes[sub_id]\n",
    "            picks = mne.pick_channels(raw.ch_names, subject_electrode_names)\n",
    "            raw = raw.pick(picks)\n",
    "        elif channel_reg_ex:\n",
    "            picks = mne.pick_channels_regexp(raw.ch_names, channel_reg_ex)\n",
    "            raw = raw.pick(picks)\n",
    "        raws.append(raw)\n",
    "\n",
    "    return raws\n",
    "\n",
    "\n",
    "def read_electrode_file(file_path: str):\n",
    "    \"\"\"\n",
    "    Parse an electrode mapping CSV file to create a subject-to-electrodes mapping.\n",
    "\n",
    "    This function reads a CSV file containing electrode information organized by subject\n",
    "    and returns a dictionary mapping each subject ID to their list of electrode names.\n",
    "    Each subject can have multiple electrodes, and the electrode order is preserved\n",
    "    as it appears in the CSV file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file containing electrode data.\n",
    "                        The CSV must have columns 'subject' (int) and 'elec' (str).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are subject IDs (int) and values are lists\n",
    "              of electrode names (str) for that subject. For example:\n",
    "              {1: ['A1', 'A2', 'B1'], 2: ['C1', 'C2']}\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the specified file does not exist.\n",
    "        KeyError: If required columns 'subject' or 'elec' are missing from the CSV.\n",
    "\n",
    "    Example:\n",
    "        >>> # CSV file contains:\n",
    "        >>> # subject,elec\n",
    "        >>> # 1,A1\n",
    "        >>> # 1,A2\n",
    "        >>> # 2,C1\n",
    "        >>> result = read_electrode_file('electrodes.csv')\n",
    "        >>> print(result)\n",
    "        {1: ['A1', 'A2'], 2: ['C1']}\n",
    "    \"\"\"\n",
    "    file_data = pd.read_csv(file_path)\n",
    "    subjects, electrodes = file_data.subject, file_data.elec\n",
    "\n",
    "    sub_elec_mapping = {}\n",
    "    for subject, electrode in zip(subjects, electrodes):\n",
    "        subject = int(subject)\n",
    "        if subject not in sub_elec_mapping.keys():\n",
    "            sub_elec_mapping[subject] = []\n",
    "\n",
    "        sub_elec_mapping[subject].append(electrode)\n",
    "\n",
    "    return sub_elec_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fd671d-e88a-4eac-89dc-a0184f7d3bb6",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f735e6a-f57b-4be4-94e6-b3472ff2ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all data.\n",
    "if electrode_file_path:\n",
    "    subject_electrode_mapping = read_electrode_file(electrode_file_path)\n",
    "else:\n",
    "    subject_electrode_mapping = None\n",
    "\n",
    "raws = data_utils.load_raws(subject_electrode_mapping)\n",
    "df_word = word_embedding_decoding_task()\n",
    "\n",
    "X, Y, selected_words = data_utils.get_data(\n",
    "            lag,\n",
    "            raws,\n",
    "            df_word,\n",
    "            window_width,\n",
    "            word_column=word_column,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bd2e4e-1fe2-4b0b-964d-e6863e15fbcc",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b35e5a-a756-4de5-88f2-97e53359e8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_metrics_and_loss():\n",
    "    \"\"\"\n",
    "    Set up metrics and loss functions from training parameters.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary mapping metric names to callable functions\n",
    "    \"\"\"\n",
    "    # Combine loss and metrics into single list\n",
    "    metric_names = losses + metrics\n",
    "\n",
    "    # Resolve all functions from registry\n",
    "    all_fns = {name: simple_metric_dict[name] for name in metric_names}\n",
    "\n",
    "    return all_fns\n",
    "\n",
    "\n",
    "def compute_loss(out, groundtruth, all_fns):\n",
    "    loss = 0.0\n",
    "    for i, loss_name in enumerate(losses):\n",
    "        loss += loss_weights[i] * all_fns[loss_name](out, groundtruth)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def validate_early_stopping_config():\n",
    "    \"\"\"\n",
    "    Validate that early stopping configuration is valid.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If early stopping metric is not in available metrics\n",
    "    \"\"\"\n",
    "    available_metrics = [loss_name] + metrics\n",
    "\n",
    "    if early_stopping_metric not in available_metrics:\n",
    "        raise ValueError(\n",
    "            f\"Early stopping metric '{early_stopping_metric}' \"\n",
    "            f\"must be either the loss function or in the metrics list. \"\n",
    "            f\"Available: {available_metrics}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def setup_early_stopping_state():\n",
    "    \"\"\"\n",
    "    Set up initial state for early stopping.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (best_val, patience) initial values\n",
    "    \"\"\"\n",
    "    if smaller_is_better:\n",
    "        best_val = float(\"inf\")\n",
    "    else:\n",
    "        best_val = -float(\"inf\")\n",
    "\n",
    "    patience = 0\n",
    "\n",
    "    return best_val, patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf1811f-7ac5-4e9f-a979-c715ce2aaca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare device & output dir\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# 2. Convert to tensors if needed\n",
    "if isinstance(X, np.ndarray):\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "if isinstance(Y, np.ndarray):\n",
    "    Y = torch.tensor(Y, dtype=torch.float32)\n",
    "\n",
    "# 3. Get fold indices\n",
    "if training_params.fold_type == \"sequential_folds\":\n",
    "    fold_indices = get_sequential_folds(X, num_folds=n_folds)\n",
    "elif training_params.fold_type == \"zero_shot_folds\":\n",
    "    fold_indices = get_zero_shot_folds(\n",
    "        selected_words, num_folds=]n_folds\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Unknown fold_type: {fold_type}\")\n",
    "\n",
    "# 4. Build a single dict of all metric functions (including loss)\n",
    "all_fns = setup_metrics_and_loss()\n",
    "metric_names = all_fns.keys()\n",
    "\n",
    "# 5. Initialize CV containers\n",
    "phases = (\"train\", \"val\", \"test\")\n",
    "cv_results = {f\"{phase}_{name}\": [] for phase in phases for name in metric_names}\n",
    "cv_results[\"num_epochs\"] = []\n",
    "\n",
    "# Hardcode embedding task metrics for now since they need to be handled a bit differently.\n",
    "# Clean this up later. Hardcoding for now since generalizing this like other metrics would\n",
    "# get complicated.\n",
    "# Test type is split between \"word\" and \"occ\" where word is averaged over\n",
    "# each time a word occurs and occ is per-each occurence of the word so is\n",
    "# more difficult and depends on contextual embeddings.\n",
    "embedding_metrics = [\n",
    "    \"test_word_avg_auc_roc\",\n",
    "    \"test_word_train_weighted_auc_roc\",\n",
    "    \"test_word_test_weighted_auc_roc\",\n",
    "    \"test_word_perplexity\",\n",
    "    \"test_occ_perplexity\",\n",
    "]\n",
    "\n",
    "# Top-K metrics.\n",
    "for k_val in top_k_thresholds:\n",
    "    for test_type in [\"word\", \"occ\"]:\n",
    "        embedding_metrics.append(f\"test_{test_type}_top_{k_val}\")\n",
    "\n",
    "for metric in embedding_metrics:\n",
    "    cv_results[metric] = []\n",
    "\n",
    "models, histories = [], []\n",
    "\n",
    "def run_epoch(model, loader, optimizer=None):\n",
    "    \"\"\"\n",
    "    If optimizer is provided: does a training pass.\n",
    "    Otherwise: does an eval pass.\n",
    "    Returns a dict { metric_name: average_value }.\n",
    "    \"\"\"\n",
    "    is_train = optimizer is not None\n",
    "    if is_train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    sums = {name: 0.0 for name in metric_names}\n",
    "    sums[\"loss\"] = 0.0\n",
    "\n",
    "    grad_steps = grad_accumulation_steps\n",
    "    if is_train:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    for i, (Xb, yb) in enumerate(loader):\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        bsz = Xb.size(0)\n",
    "\n",
    "        if is_train:\n",
    "            out = model(Xb)\n",
    "            loss = compute_loss(out, yb, training_params, all_fns)\n",
    "            # Normalize loss to account for gradient accumulation\n",
    "            loss = loss / grad_steps\n",
    "            loss.backward()\n",
    "\n",
    "            if should_update_gradient_accumulation(i, len(loader), grad_steps):\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                out = model(Xb)\n",
    "                loss = compute_loss(out, yb, training_params, all_fns)\n",
    "\n",
    "        # accumulate each metric\n",
    "        for name, fn in all_fns.items():\n",
    "            val = fn(out, yb)\n",
    "            # get a scalar float\n",
    "            if torch.is_tensor(val):\n",
    "                val = val.detach().mean().item()\n",
    "            sums[name] += val\n",
    "\n",
    "        # add loss to sums\n",
    "        if torch.is_tensor(loss):\n",
    "            loss = loss.detach().mean().item()\n",
    "        sums[\"loss\"] += loss\n",
    "    return {name: sums[name] / len(loader) for name in sums}\n",
    "\n",
    "# 6. Cross‐val loop\n",
    "for fold, (tr_idx, va_idx, te_idx) in enumerate(fold_indices, start=1):\n",
    "    model_path = os.path.join(model_dir, f\"best_model_fold{fold}.pt\")\n",
    "\n",
    "    # DataLoaders\n",
    "    datasets = {\n",
    "        \"train\": TensorDataset(X[tr_idx], Y[tr_idx]),\n",
    "        \"val\": TensorDataset(X[va_idx], Y[va_idx]),\n",
    "        \"test\": TensorDataset(X[te_idx], Y[te_idx]),\n",
    "    }\n",
    "    loaders = {\n",
    "        phase: DataLoader(\n",
    "            ds, batch_size=batch_size, shuffle=(phase == \"train\")\n",
    "        )\n",
    "        for phase, ds in datasets.items()\n",
    "    }\n",
    "\n",
    "    # Model, optimizer, early‐stop setup\n",
    "    model = get_model().to(device)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "    )\n",
    "\n",
    "    best_val, patience = setup_early_stopping_state(training_params)\n",
    "    best_epoch = 0\n",
    "\n",
    "    # per‐fold history (only train & val, for plotting)\n",
    "    history = {\n",
    "        f\"{phase}_{name}\": [] for phase in (\"train\", \"val\") for name in metric_names\n",
    "    }\n",
    "    history[\"train_loss\"] = []\n",
    "    history[\"val_loss\"] = []\n",
    "    history[\"num_epochs\"] = None\n",
    "\n",
    "    loop = tqdm(range(epochs), desc=f\"Lag {lag}, Fold {fold}\")\n",
    "    for epoch in loop:\n",
    "        train_mets = run_epoch(model, loaders[\"train\"], optimizer)\n",
    "        val_mets = run_epoch(model, loaders[\"val\"])\n",
    "\n",
    "        # record + TensorBoard\n",
    "        for name, val in train_mets.items():\n",
    "            history[f\"train_{name}\"].append(val)\n",
    "        for name, val in val_mets.items():\n",
    "            history[f\"val_{name}\"].append(val)\n",
    "\n",
    "        # early stopping on requested metric\n",
    "        cur = val_mets[early_stopping_metric]\n",
    "        if should_update_best(cur, best_val, smaller_is_better):\n",
    "            best_val = cur\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= early_stopping_patience:\n",
    "                break\n",
    "\n",
    "        loop.set_postfix(\n",
    "            {\n",
    "                early_stopping_metric: f\"{best_val:.4f}\",\n",
    "                **{f\"train_{name}\": val for name, val in train_mets.items()},\n",
    "                **{f\"val_{name}\": val for name, val in val_mets.items()},\n",
    "            }\n",
    "        )\n",
    "\n",
    "    history[\"num_epochs\"] = best_epoch + 1\n",
    "\n",
    "    # load best and eval on test set\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    test_mets = run_epoch(model, loaders[\"test\"])\n",
    "\n",
    "    # record into cv_results\n",
    "    for name in metric_names:\n",
    "        cv_results[f\"train_{name}\"].append(history[f\"train_{name}\"][best_epoch])\n",
    "        cv_results[f\"val_{name}\"].append(history[f\"val_{name}\"][best_epoch])\n",
    "        cv_results[f\"test_{name}\"].append(test_mets[name])\n",
    "    cv_results[\"num_epochs\"].append(history[\"num_epochs\"])\n",
    "\n",
    "    # word‐level ROC and top-k. Only useful for word embedding task.\n",
    "    # Hardcoded for now since this would be a bit complicated\n",
    "    # to generalize at the moment.\n",
    "    results = compute_word_embedding_task_metrics(\n",
    "        X[te_idx],\n",
    "        Y[te_idx],\n",
    "        model,\n",
    "        device,\n",
    "        selected_words,\n",
    "        te_idx,\n",
    "        tr_idx,\n",
    "        top_k_thresholds,\n",
    "        min_train_freq_auc,\n",
    "        min_test_freq_auc,\n",
    "    )\n",
    "    for key, val in results.items():\n",
    "        cv_results[key].append(val)\n",
    "\n",
    "    models.append(model)\n",
    "    histories.append(history)\n",
    "\n",
    "    if plot_results:\n",
    "        plot_training_history(history, fold=fold)\n",
    "\n",
    "# 7. Print CV summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CROSS-VALIDATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "for phase in phases:\n",
    "    for name in metric_names:\n",
    "        vals = cv_results[f\"{phase}_{name}\"]\n",
    "        print(f\"Mean {phase} {name}: {np.mean(vals):.4f} ± {np.std(vals):.4f}\")\n",
    "for metric_name in embedding_metrics:\n",
    "    vals = cv_results[metric_name]\n",
    "    print(f\"Mean {metric_name}: {np.mean(vals):.4f} ± {np.std(vals):.4f}\")\n",
    "\n",
    "if plot_results:\n",
    "    plot_cv_results(cv_results)\n",
    "\n",
    "# Key outputs: models, histories, cv_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
